# -*- coding: utf-8 -*-
"""Real time news sentiment.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Nix86feH7md8AoBCSeMb5EPN7rkDw0gb
"""


# -*- coding: utf-8 -*-
"""
Real-Time News Sentiment Dashboard with PySpark + Streamlit
"""

import os
import time
import uuid
import pandas as pd
import streamlit as st
import plotly.express as px
import requests

from pathlib import Path

# PySpark imports
from pyspark.sql import SparkSession
from pyspark.ml import Pipeline, PipelineModel
from pyspark.ml.feature import Tokenizer, HashingTF, StringIndexer
from pyspark.ml.classification import LogisticRegression

# Optional fallback news
try:
    from gnews import GNews
except:
    os.system("pip install gnews")
    from gnews import GNews

# ===================== CONFIG =====================
NEWSAPI_KEY = "YOUR_NEWSAPI_KEY"  # <-- Replace with your NewsAPI key
MODEL_PATH = "news_sentiment_pipeline"
PRED_DIR = Path("predictions_parquet")

os.environ["JAVA_HOME"] = "/usr/lib/jvm/java-11-openjdk-amd64"

# ===================== INIT SPARK =====================
spark = SparkSession.builder \
    .appName("RealTimeNewsSentiment") \
    .master("local[*]") \
    .config("spark.sql.shuffle.partitions", "2") \
    .getOrCreate()

# ===================== TRAIN PIPELINE IF NOT EXIST =====================
if not Path(MODEL_PATH).exists():
    st.warning("Training PySpark sentiment pipeline...")
    # Small sample training data
    data = [
        ("I love this product", "positive"),
        ("This is the worst experience", "negative"),
        ("Amazing news today!", "positive"),
        ("I hate delays", "negative"),
        ("Great performance and quality", "positive"),
        ("Terrible customer service", "negative")
    ]
    df_train = spark.createDataFrame(data, ["text", "label"])
    indexer = StringIndexer(inputCol="label", outputCol="labelIndex")
    tokenizer = Tokenizer(inputCol="text", outputCol="words")
    hashingTF = HashingTF(inputCol="words", outputCol="rawFeatures")
    lr = LogisticRegression(featuresCol="rawFeatures", labelCol="labelIndex")
    pipeline = Pipeline(stages=[indexer, tokenizer, hashingTF, lr])
    model = pipeline.fit(df_train)
    model.write().overwrite().save(MODEL_PATH)
    st.success("Pipeline trained and saved!")
else:
    model = PipelineModel.load(MODEL_PATH)

# Get label map
try:
    label_map = model.stages[0].labels
except:
    label_map = ["negative", "positive"]

# ===================== NEWS FETCHERS =====================
def fetch_news_newsapi(limit=20):
    url = "https://newsapi.org/v2/top-headlines"
    params = {
        "apiKey": NEWSAPI_KEY,
        "language": "en",
        "pageSize": limit
    }
    try:
        r = requests.get(url, params=params, timeout=10)
        r.raise_for_status()
        articles = r.json().get("articles", [])
        rows = []
        for a in articles:
            rows.append({
                "id": a.get("url"),
                "source": (a.get("source") or {}).get("name"),
                "title": a.get("title"),
                "publishedAt": a.get("publishedAt")
            })
        return pd.DataFrame(rows)
    except:
        st.warning("NewsAPI failed. Using GNews fallback.")
        return fetch_news_gnews(limit)

def fetch_news_gnews(limit=20):
    g = GNews(language="en", country="US")
    articles = g.get_top_news()[:limit]
    rows = []
    for a in articles:
        rows.append({
            "id": a.get("url"),
            "source": a.get("source"),
            "title": a.get("title"),
            "publishedAt": a.get("publishedAt")
        })
    return pd.DataFrame(rows)

# ===================== PREDICTION =====================
def batch_predict_and_append(df_pandas):
    if df_pandas.empty:
        return None
    if 'id' not in df_pandas.columns:
        df_pandas['id'] = [str(uuid.uuid4()) for _ in range(len(df_pandas))]
    df_pandas = df_pandas.rename(columns={"title":"text"})
    sdf = spark.createDataFrame(df_pandas[["id","source","text","publishedAt"]])
    preds = model.transform(sdf)
    out = preds.select("id","source","text","prediction","probability","publishedAt").toPandas()
    out['prob_list'] = out['probability'].apply(lambda v: list(v))
    pos_idx = label_map.index("positive") if "positive" in label_map else 1
    out['prob_pos'] = out['prob_list'].apply(lambda lst: float(lst[pos_idx]) if lst else None)
    out['sentiment'] = out['prediction'].apply(lambda x: label_map[int(x)])
    os.makedirs(PRED_DIR, exist_ok=True)
    fname = PRED_DIR / f"pred_{uuid.uuid4().hex}.parquet"
    out[['id','source','text','publishedAt','sentiment','prob_pos']].to_parquet(fname, index=False)
    return out

# ===================== DASHBOARD =====================
st.set_page_config(page_title="Real-Time News Sentiment", layout="wide")
st.title("ðŸ“° Real-Time News Sentiment Dashboard")

refresh_interval = st.sidebar.slider("Refresh interval (seconds)", 10, 120, 30)

if st.button("Fetch & Classify Latest News"):
    df_new = fetch_news_newsapi(20)
    out = batch_predict_and_append(df_new)
    st.success(f"Processed {len(out)} headlines") if out is not None else st.warning("No headlines fetched")

def load_recent(n=200):
    files = sorted(PRED_DIR.glob("*.parquet"), key=lambda p: p.stat().st_mtime, reverse=True)[:50]
    if not files:
        return pd.DataFrame(columns=["id","source","text","publishedAt","sentiment","prob_pos"])
    df = pd.concat([pd.read_parquet(f) for f in files], ignore_index=True)
    df = df.drop_duplicates(subset=['id'])
    df['publishedAt'] = pd.to_datetime(df['publishedAt'], errors='coerce')
    return df.sort_values('publishedAt', ascending=False).head(n)

df = load_recent()
st.subheader("Latest Headlines")
st.dataframe(df[['publishedAt','source','text','sentiment','prob_pos']].rename(columns={'text':'headline'}), height=400)

col1, col2 = st.columns([2,1])
with col1:
    st.subheader("Sentiment Distribution")
    if not df.empty:
        counts = df['sentiment'].value_counts().rename_axis('sentiment').reset_index(name='count')
        fig = px.bar(counts, x='sentiment', y='count', color='sentiment', title="Sentiment Distribution")
        st.plotly_chart(fig, use_container_width=True)
with col2:
    st.subheader("Positive Sentiment Trend")
    if not df.empty:
        df_sorted = df.sort_values('publishedAt')
        st.line_chart(df_sorted.set_index('publishedAt')['prob_pos'].fillna(0))

st.markdown(f"**Last updated:** {pd.Timestamp.now()}")
